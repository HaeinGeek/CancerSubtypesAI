{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ge-FeojEfh_d",
    "outputId": "2764d721-aada-4149-9118-ff33b639cfb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CancerSubtypesAI'...\n",
      "remote: Enumerating objects: 905, done.\u001b[K\n",
      "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 905 (delta 23), reused 38 (delta 20), pack-reused 856 (from 1)\u001b[K\n",
      "Receiving objects: 100% (905/905), 83.84 MiB | 22.71 MiB/s, done.\n",
      "Resolving deltas: 100% (493/493), done.\n",
      "Updating files: 100% (58/58), done.\n",
      "Downloading data/processed/embeddings/mut_embeddings_mean_max_cls_1.zip (138 MB)\n",
      "Error downloading object: data/processed/embeddings/mut_embeddings_mean_max_cls_1.zip (d6bca3f): Smudge error: Error downloading data/processed/embeddings/mut_embeddings_mean_max_cls_1.zip (d6bca3f495793afd9b1c3027571dc3daec4558e9646f43eae0e40097294bbe09): batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
      "\n",
      "Errors logged to /content/CancerSubtypesAI/.git/lfs/logs/20241020T023855.215762571.log\n",
      "Use `git lfs logs last` to view the log.\n",
      "error: external filter 'git-lfs filter-process' failed\n",
      "fatal: data/processed/embeddings/mut_embeddings_mean_max_cls_1.zip: smudge filter lfs failed\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. GitHub 레포지토리 설치\n",
    "!git clone https://github.com/HaeinGeek/CancerSubtypesAI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WojEvNuqfje3",
    "outputId": "1914e4f5-b2e7-40dd-8b4f-ac98e03391f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CancerSubtypesAI\n"
     ]
    }
   ],
   "source": [
    "# 2. 작업 디렉터리 이동\n",
    "%cd CancerSubtypesAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31zBAVYffjcb"
   },
   "outputs": [],
   "source": [
    "# 3. PYTHONPATH 설정 (processing 모듈을 패키지로 인식하도록)\n",
    "import sys\n",
    "sys.path.append(\"/content/CancerSubtypesAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uln5PKns2TAY"
   },
   "outputs": [],
   "source": [
    "# 4. data 업로드\n",
    "import os\n",
    "upload_dir = 'data/processed'  # 원하는 디렉토리 경로\n",
    "os.makedirs(upload_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSxVEXCV2aPT"
   },
   "outputs": [],
   "source": [
    "# 파일 업로드\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqA7uolC20Yg"
   },
   "outputs": [],
   "source": [
    "# 업로드된 파일을 지정 디렉토리로 이동\n",
    "for filename in uploaded.keys():\n",
    "    file_path = os.path.join(upload_dir, filename)\n",
    "    print(f'Moved {filename} to {file_path}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR77uwTv5Yow"
   },
   "source": [
    "# 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92uRMTjdbZAY",
    "outputId": "a59b236a-96c4-43b1-dd71-235dbe466c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/train/input_data.json에서 모델 입력 데이터를 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "# 저장된 모델 입력 데이터 불러오기\n",
    "train_input = load_model_input('data/processed/train/input_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-vRGo0fbZGR"
   },
   "outputs": [],
   "source": [
    "# 결과 저장 경로 설정\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "os.makedirs('./logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I14X0h4hcuLp",
    "outputId": "796960ea-5f3c-46bd-febc-7fd7528319b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['KIPAN', 'SARC', 'SKCM', 'KIRC', 'GBMLGG', 'STES', 'BRCA', 'THCA',\n",
       "       'LIHC', 'HNSC', 'PAAD', 'OV', 'PRAD', 'UCEC', 'LAML', 'COAD',\n",
       "       'ACC', 'LGG', 'LUSC', 'LUAD', 'CESC', 'PCPG', 'THYM', 'BLCA',\n",
       "       'TGCT', 'DLBC'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['SUBCLASS'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUu_BS0wUtz3"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjAcNHP7XgG9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWPHIvC0bZCV"
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "SCHEDULER_PATIENCE = 3\n",
    "SCHEDULER_FACTOR = 0.5\n",
    "VAL_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "DROPOUT_RATE = 0.5\n",
    "HIDDEN_LAYERS = [512, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du34Q0T5bY-b"
   },
   "outputs": [],
   "source": [
    "# SUBCLASS를 인덱스로 매핑\n",
    "subclasses = ['KIPAN', 'SARC', 'SKCM', 'KIRC', 'GBMLGG', 'STES', 'BRCA', 'THCA',\n",
    "       'LIHC', 'HNSC', 'PAAD', 'OV', 'PRAD', 'UCEC', 'LAML', 'COAD',\n",
    "       'ACC', 'LGG', 'LUSC', 'LUAD', 'CESC', 'PCPG', 'THYM', 'BLCA',\n",
    "       'TGCT', 'DLBC']\n",
    "subclass_to_idx = {subclass: idx for idx, subclass in enumerate(sorted(subclasses))}\n",
    "idx_to_subclass = {idx: subclass for subclass, idx in subclass_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWmRDGCOcns2"
   },
   "outputs": [],
   "source": [
    "# 돌연변이 유형을 인덱스로 매핑\n",
    "mutation_types = ['WT', 'Silent_Missense', 'Missense', 'Nonsense',\n",
    "       'Complex_mutation', 'Frameshift', 'Silent_Nonsense', 'Deletion',\n",
    "       'Insertion', 'Delins', 'Unknown']\n",
    "mutation_type_to_idx = {mt: idx for idx, mt in enumerate(mutation_types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwXU0IPh-oSd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, data, subclass_to_idx, mutation_type_to_idx, feature_mean=None, feature_std=None, normalize=False):\n",
    "        self.data = data\n",
    "        self.subclass_to_idx = subclass_to_idx\n",
    "        self.mutation_type_to_idx = mutation_type_to_idx\n",
    "        self.normalize = normalize\n",
    "        self.feature_mean = feature_mean\n",
    "        self.feature_std = feature_std\n",
    "\n",
    "        # 정규화 시 제외할 특성 인덱스 (나중에 계산)\n",
    "        self.exclude_norm_indices = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        features = []\n",
    "\n",
    "        # --- 수치형 변수 ---\n",
    "        # num_mutated_genes\n",
    "        mutation_stats = sample.get('mutation_stats') or {}\n",
    "        num_mutated_genes = mutation_stats.get('num_mutated_genes', 1)  # 0으로 나누는 것을 방지하기 위해 1로 기본값 설정\n",
    "        features.append(num_mutated_genes)\n",
    "\n",
    "        # additional_stats (status_ratio, status_prot_ratio는 제외)\n",
    "        additional_stats = sample.get('additional_stats') or {}\n",
    "        avg_mut_num = additional_stats.get('avg_mut_num', 0)\n",
    "        max_mut_num = additional_stats.get('max_mut_num', 0)\n",
    "        features.extend([avg_mut_num, max_mut_num])\n",
    "        # status_ratio와 status_prot_ratio는 별도로 저장\n",
    "        status_ratio = additional_stats.get('status_ratio', 0)\n",
    "        status_prot_ratio = additional_stats.get('status_prot_ratio', 0)\n",
    "\n",
    "        # aa_change_stats\n",
    "        aa_change_stats = sample.get('aa_change_stats') or {}\n",
    "        for prop in ['hydrophobicity', 'polarity', 'mw', 'pI', 'charge']:\n",
    "            prop_stats = aa_change_stats.get(prop) or {}\n",
    "            for stat in ['{}_min', '{}_max', '{}_mean', '{}_std']:\n",
    "                value = prop_stats.get(stat.format(prop), 0)\n",
    "                features.append(value)\n",
    "\n",
    "        # 임베딩 통계량\n",
    "        embedding_stats = sample.get('embedding_stats') or {}\n",
    "        embedding_size = len(embedding_stats.get('mean', []))\n",
    "        for stat in ['mean', 'min', 'max', 'std']:\n",
    "            embedding_values = embedding_stats.get(stat, None)\n",
    "            if embedding_values is not None and len(embedding_values) == embedding_size:\n",
    "                features.extend(embedding_values)\n",
    "            else:\n",
    "                features.extend([0.0] * embedding_size)\n",
    "\n",
    "        # --- 변이 유형 빈도 비율 특성 ---\n",
    "        mutation_type_freq = mutation_stats.get('mutation_type_freq', {})\n",
    "        mutation_type_features = [0.0] * len(self.mutation_type_to_idx)\n",
    "        for mt, count in mutation_type_freq.items():\n",
    "            idx_mt = self.mutation_type_to_idx.get(mt)\n",
    "            if idx_mt is not None:\n",
    "                # 변이 유형 빈도 비율 계산\n",
    "                mutation_type_features[idx_mt] = count / num_mutated_genes  # num_mutated_genes가 0인 경우는 없음 (1로 설정)\n",
    "        # mutation_type_features는 정규화 대상에서 제외\n",
    "        features.extend(mutation_type_features)\n",
    "\n",
    "        # --- 범주형 변수 및 정규화 제외 변수 ---\n",
    "        # status_ratio와 status_prot_ratio 추가\n",
    "        features.extend([status_ratio, status_prot_ratio])\n",
    "        # subclass는 라벨로 처리\n",
    "\n",
    "        # 특성을 텐서로 변환\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "        # 정규화 대상에서 제외할 인덱스 계산 (한 번만 수행)\n",
    "        if self.exclude_norm_indices is None:\n",
    "            total_features = len(features)\n",
    "            # status_ratio와 status_prot_ratio의 인덱스\n",
    "            status_indices = [-2, -1]  # 마지막 두 요소\n",
    "            # mutation_type_features의 시작 인덱스와 종료 인덱스\n",
    "            mutation_type_start = total_features - len(self.mutation_type_to_idx) - 2  # -2는 status_ratio와 status_prot_ratio\n",
    "            mutation_type_end = mutation_type_start + len(self.mutation_type_to_idx)\n",
    "            # 정규화 제외 인덱스\n",
    "            self.exclude_norm_indices = status_indices + list(range(mutation_type_start, mutation_type_end))\n",
    "\n",
    "        # 정규화\n",
    "        if self.normalize and self.feature_mean is not None and self.feature_std is not None:\n",
    "            indices = [i for i in range(len(features)) if i not in self.exclude_norm_indices]\n",
    "            features_to_normalize = features[indices]\n",
    "            normalized_features = (features_to_normalize - self.feature_mean) / self.feature_std\n",
    "            features[indices] = normalized_features\n",
    "\n",
    "        # 라벨 추출\n",
    "        subclass = sample.get('subclass', None)\n",
    "        if subclass is not None:\n",
    "            label = self.subclass_to_idx[subclass]\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            label = torch.tensor(-1, dtype=torch.long)\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WS3mlNabYyY"
   },
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class CancerModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_layers, dropout_rate):\n",
    "        super(CancerModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmtqXLKI-vBE"
   },
   "outputs": [],
   "source": [
    "# # 데이터 분할\n",
    "train_data, val_data = train_test_split(train_input, test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "# 훈련 데이터의 특성 평균과 표준편차 계산\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 임시로 normalize=False로 데이터셋 생성하여 전체 특성 수 파악\n",
    "temp_dataset = CancerDataset(train_data, subclass_to_idx, mutation_type_to_idx, normalize=False)\n",
    "temp_loader = DataLoader(temp_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "all_features = []\n",
    "for features, _ in temp_loader:\n",
    "    all_features.append(features)\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "# 정규화 대상 인덱스 추출\n",
    "exclude_indices = temp_dataset.exclude_norm_indices\n",
    "include_indices = [i for i in range(all_features.shape[1]) if i not in exclude_indices]\n",
    "\n",
    "# 정규화 대상 변수들에 대해 평균과 표준편차 계산\n",
    "features_to_normalize = all_features[:, include_indices]\n",
    "feature_mean = features_to_normalize.mean(dim=0)\n",
    "feature_std = features_to_normalize.std(dim=0)\n",
    "# 표준편차가 0인 경우 대비\n",
    "feature_std[feature_std == 0] = 1\n",
    "\n",
    "# 최종 데이터셋 초기화\n",
    "train_dataset = CancerDataset(train_data, subclass_to_idx, mutation_type_to_idx,\n",
    "                              feature_mean=feature_mean, feature_std=feature_std, normalize=True)\n",
    "val_dataset = CancerDataset(val_data, subclass_to_idx, mutation_type_to_idx,\n",
    "                            feature_mean=feature_mean, feature_std=feature_std, normalize=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef7anv46bYs7"
   },
   "outputs": [],
   "source": [
    "# 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvZAeCuEbYq_"
   },
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CancerModel(input_size, num_classes, HIDDEN_LAYERS, DROPOUT_RATE)\n",
    "model = model.to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=SCHEDULER_PATIENCE, factor=SCHEDULER_FACTOR, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8neAKY0pbYpE",
    "outputId": "185e8c81-78d4-47e6-851b-7ef298ce0349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Train Loss: nan Acc: 0.0129 F1: 0.0034 Val Loss: nan Acc: 0.0113 F1: 0.0009\n",
      "Epoch 2/50 Train Loss: nan Acc: 0.0117 F1: 0.0009 Val Loss: nan Acc: 0.0113 F1: 0.0009\n",
      "Epoch 3/50 Train Loss: nan Acc: 0.0117 F1: 0.0009 Val Loss: nan Acc: 0.0113 F1: 0.0009\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# 학습 기록 저장을 위한 리스트\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "best_val_loss = np.inf\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for features, labels in train_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    train_f1_scores.append(epoch_f1)\n",
    "\n",
    "    # 검증 단계\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * features.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "    val_loss = val_running_loss / len(val_dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_f1 = f1_score(val_all_labels, val_all_preds, average='macro')\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # 스케줄러에 검증 손실 전달\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print('Epoch {}/{} Train Loss: {:.4f} Acc: {:.4f} F1: {:.4f} Val Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(\n",
    "        epoch+1, NUM_EPOCHS, epoch_loss, epoch_acc, epoch_f1, val_loss, val_acc, val_f1))\n",
    "\n",
    "    # 검증 손실이 개선되었는지 확인\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # 모델 체크포인트 저장\n",
    "        checkpoint_path = f'/checkpoints/best_model_epoch_{epoch+1}.pt'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "            print('Early stopping triggered.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYq21Nsv5uZU"
   },
   "source": [
    "## 학습결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PN46YlpabYmv"
   },
   "outputs": [],
   "source": [
    "# 로그 저장\n",
    "log_data = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'train_f1_scores': train_f1_scores,\n",
    "    'val_f1_scores': val_f1_scores\n",
    "}\n",
    "log_path = '/logs/training_logs.npy'\n",
    "np.save(log_path, log_data)\n",
    "print(f'Training logs saved at {log_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXzzKIWBXgEH"
   },
   "outputs": [],
   "source": [
    "# 시각화\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss per Epoch')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label='Train Acc')\n",
    "plt.plot(epochs_range, val_accuracies, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy per Epoch')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs_range, train_f1_scores, label='Train F1')\n",
    "plt.plot(epochs_range, val_f1_scores, label='Val F1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.legend()\n",
    "plt.title('Macro F1 Score per Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/logs/training_plots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzGgEE5f5xfo"
   },
   "source": [
    "## 테스트 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8oUN1STbQMU"
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 예측\n",
    "# 테스트 데이터 로드\n",
    "test_input = load_model_input('data/processed/test/input_data.json')\n",
    "\n",
    "test_dataset = CancerDataset(test_input, subclass_to_idx, mutation_type_to_idx)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 가장 좋은 모델 로드\n",
    "best_model_path = checkpoint_path  # 이전에 저장된 가장 좋은 모델의 경로\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_ids = []\n",
    "sample_idx = 0  # 전체 데이터셋에서의 샘플 인덱스 초기화\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "        # 현재 배치의 샘플 수를 계산\n",
    "        batch_size = features.size(0)\n",
    "\n",
    "        # 현재 배치의 샘플에 해당하는 ID를 추출\n",
    "        batch_samples = test_input[sample_idx : sample_idx + batch_size]\n",
    "        for sample in batch_samples:\n",
    "            test_ids.append(sample['id'])\n",
    "\n",
    "        # 전체 샘플 인덱스 업데이트\n",
    "        sample_idx += batch_size\n",
    "\n",
    "# 예측 결과를 ID와 함께 저장\n",
    "test_results = []\n",
    "for idx, pred in zip(test_ids, test_predictions):\n",
    "    subclass_pred = idx_to_subclass[pred]\n",
    "    test_results.append({'id': idx, 'predicted_subclass': subclass_pred})\n",
    "\n",
    "# 결과를 파일로 저장\n",
    "with open('test_predictions.json', 'w') as f:\n",
    "    json.dump(test_results, f)\n",
    "print('Test predictions saved to test_predictions.json')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "63c44gOPWyAo"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
